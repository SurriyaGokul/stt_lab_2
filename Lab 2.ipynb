{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdd9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name microsoft/codebert-base. Creating a new one with mean pooling.\n",
      "c:\\Users\\Surrya Gokul\\anaconda3\\envs\\genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Surrya Gokul\\.cache\\huggingface\\hub\\models--microsoft--codebert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Surrya Gokul\\anaconda3\\envs\\genai\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Surrya Gokul\\.cache\\huggingface\\hub\\models--mamiksik--CommitPredictorT5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# ---------- 2. Load models ----------\n",
    "# Model for semantic similarity\n",
    "sim_model = SentenceTransformer(\"microsoft/codebert-base\")\n",
    "\n",
    "# Commit predictor model (for rectification)\n",
    "pred_model_name = \"mamiksik/CommitPredictorT5\"\n",
    "pred_tokenizer = AutoTokenizer.from_pretrained(pred_model_name)\n",
    "pred_model = AutoModelForSeq2SeqLM.from_pretrained(pred_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aab894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating commits: 100%|██████████| 979/979 [1:51:09<00:00,  6.81s/it]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pred_model = pred_model.to(device)\n",
    "\n",
    "def batch_rectify_messages(diffs, batch_size=16):\n",
    "    cleaned = [clean_diff(d) for d in diffs]\n",
    "    results = []\n",
    "\n",
    "    # tqdm gives ETA automatically\n",
    "    for i in tqdm(range(0, len(cleaned), batch_size), desc=\"Generating commits\"):\n",
    "        batch = cleaned[i:i+batch_size]\n",
    "        inputs = pred_tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = pred_model.generate(**inputs, max_length=64)\n",
    "\n",
    "        decoded = pred_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        results.extend(decoded)\n",
    "\n",
    "    return results\n",
    "\n",
    "df[\"LLM Inference\"] = batch_rectify_messages(df[\"Diff\"].tolist(), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"llm_inference.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
